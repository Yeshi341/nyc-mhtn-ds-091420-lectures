{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods Applied\n",
    "\n",
    "Agenda:\n",
    "- Review code for Voting Classifier, Bagging Classifier, and Random Forest\n",
    "- Practice finding optimal hyperparameter for  Random Forest with gridsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Prep Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import BaggingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/nyc-ds-033020-lectures/master/Mod_3/decision_trees/cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use x and y variables to split the training data into train and test set then scale that data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7975460122699386\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_preds = knn.predict(X_test)\n",
    "\n",
    "knn_f1 = metrics.f1_score(y_test, knn_preds)\n",
    "\n",
    "\n",
    "print(knn_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Logistic Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8066298342541436\n"
     ]
    }
   ],
   "source": [
    "lr_preds = lr.predict(X_test)\n",
    "\n",
    "lr_f1 = metrics.f1_score(y_test, lr_preds)\n",
    "\n",
    "print(lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8047337278106509\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=5, class_weight='balanced')\n",
    "\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "dtc_preds  = dtc.predict(X_test)\n",
    "\n",
    "dtc_f1 = metrics.f1_score(y_test, dtc_preds)\n",
    "\n",
    "print(dtc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine three models using Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the estimators, we must provide a list of tuples. The first value in the tuple is is the name given to the model/estimator in the second value. SKlearn requires this because there is additional functionality where you can access information about the specific models, so you need to name the models to access them later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8160919540229884\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr), ('knneighbors', knn), ('decisiontree', dtc)], \n",
    "                voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "vc_preds = voting_clf.predict(X_test)\n",
    "\n",
    "vc_f1 = metrics.f1_score(y_test, vc_preds)\n",
    "\n",
    "print(vc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a voting classifier with multiple Logistic regression models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C_param_range = [0.001,0.01,0.1,0.5,1]\n",
    "titles = ['lr_0_001', 'lr_0_01', 'lr_0_1', 'lr_0.5', 'lr_1']\n",
    "\n",
    "params = dict(zip(titles, C_param_range)) \n",
    "models = {}\n",
    "\n",
    "table = pd.DataFrame(columns = ['C_parameter','F1'])\n",
    "table['C_parameter'] = C_param_range\n",
    "j = 0\n",
    "\n",
    "for k , v  in params.items():\n",
    "    \n",
    "    # Create model using different value for c  \n",
    "    lr = LogisticRegression(penalty = 'l2', C = v, random_state = 1, class_weight='balanced')\n",
    "    \n",
    "    #save the model to a dictionary to use later in our voting classifiers\n",
    "    models[k]= lr\n",
    "    \n",
    "    #the steps below this point are unnecessary in order to create a voting classifier, \n",
    "    #but it is easy to fit the model and see how performance changes for different levels of regularization\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict using model\n",
    "    y_preds = lr.predict(X_test)\n",
    "\n",
    "    # Saving accuracy score in table\n",
    "    table.iloc[j,1] = metrics.f1_score(y_test, y_preds)\n",
    "    j += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_parameter</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.735135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.751381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.804469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.80663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.80663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_parameter        F1\n",
       "0        0.001  0.735135\n",
       "1        0.010  0.751381\n",
       "2        0.100  0.804469\n",
       "3        0.500   0.80663\n",
       "4        1.000   0.80663"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review performance for different levels of C\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lr_0_001',\n",
       "  LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                     max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                     random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False)),\n",
       " ('lr_0_01',\n",
       "  LogisticRegression(C=0.01, class_weight='balanced', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                     max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                     random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False)),\n",
       " ('lr_0_1',\n",
       "  LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                     max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                     random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False)),\n",
       " ('lr_1',\n",
       "  LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                     multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                     random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False)),\n",
       " ('lr_10',\n",
       "  LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                     max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                     random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invesitgate the models D=dictionary\n",
    "list(models.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have programmatically created multiple logistic regression models, let's use them in an ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "lr_voting = VotingClassifier(estimators=list(models.items()), \n",
    "                              voting='hard')\n",
    "\n",
    "lr_voting.fit(X_train, y_train)\n",
    "\n",
    "lrv_preds = lr_voting.predict(X_test)\n",
    "\n",
    "lrv_f1 = metrics.f1_score(y_test, lrv_preds)\n",
    "\n",
    "print(lrv_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so much better than our best log reg model before. because even though using different C params , it is the same obs so the results is not so much different from the indiv logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Bagging Classifier for a Logistic Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_lr = BaggingClassifier(\n",
    "            base_estimator=LogisticRegression(random_state = 1, class_weight='balanced'), \n",
    "            n_estimators= 1000,\n",
    "            max_samples= 0.8,\n",
    "            max_features= 8,\n",
    "            oob_score= True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_knn = BaggingClassifier(\n",
    "            base_estimator=KNeighborsClassifier(n_neighbors=9), \n",
    "            n_estimators= 1000,\n",
    "            max_samples= 0.8,\n",
    "            max_features= 8,\n",
    "            oob_score= True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_dtc = BaggingClassifier(\n",
    "            base_estimator=DecisionTreeClassifier(random_state = 1, max_depth = 5, class_weight='balanced'), \n",
    "            n_estimators= 1000,\n",
    "            max_samples= 0.8,\n",
    "            max_features= 8,\n",
    "            oob_score= True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_bagged = VotingClassifier(\n",
    "                estimators=[('bagged_logreg', bc_lr), ('bagged_knneighbors', bc_knn), ('bagged_decisiontree', bc_dtc)], \n",
    "                voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('bagged_logreg',\n",
       "                              BaggingClassifier(base_estimator=LogisticRegression(class_weight='balanced',\n",
       "                                                                                  random_state=1),\n",
       "                                                max_features=8, max_samples=0.8,\n",
       "                                                n_estimators=1000,\n",
       "                                                oob_score=True)),\n",
       "                             ('bagged_knneighbors',\n",
       "                              BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=9),\n",
       "                                                max_features=8, max_samples=0.8,\n",
       "                                                n_estimators=1000,\n",
       "                                                oob_score=True)),\n",
       "                             ('bagged_decisiontree',\n",
       "                              BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced',\n",
       "                                                                                      max_depth=5,\n",
       "                                                                                      random_state=1),\n",
       "                                                max_features=8, max_samples=0.8,\n",
       "                                                n_estimators=1000,\n",
       "                                                oob_score=True))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf_bagged.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8208092485549133\n"
     ]
    }
   ],
   "source": [
    "voting_clf_bagged_preds = voting_clf_bagged.predict(X_test)\n",
    "\n",
    "voting_clf_bagged_preds_f1 = metrics.f1_score(y_test, voting_clf_bagged_preds)\n",
    "\n",
    "print(voting_clf_bagged_preds_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_bagged()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(class_weight='balanced',\n",
       "                                                    random_state=1),\n",
       "                  max_features=8, max_samples=0.8, n_estimators=1000,\n",
       "                  oob_score=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_lr.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7792792792792793"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the oob_score to get some idea of how the model performs on a validation set\n",
    "\n",
    "bc_lr.oob_score_\n",
    "# Could be an acuracy score and not a f1 score... no specification in param\n",
    "#so uses the more common scoring method for the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8066298342541436\n"
     ]
    }
   ],
   "source": [
    "# See how the model performs on the test set\n",
    "\n",
    "bc_lr_preds = bc_lr.predict(X_test)\n",
    "\n",
    "bc_lr_f1 = metrics.f1_score(y_test, bc_lr_preds)\n",
    "\n",
    "print(bc_lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is the difference in the `VotingClassifier` algorithm and the `BaggingClassifier` algorithm?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer: the difference between the Voting Classifier (slightly different models on the same observations) and BaggingClassifier is a model on the base_estimator run n_estimators times with different samples set in max_samples and with random feature with each model set with the max_features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the difference between a BaggingClassifier that uses a decision tree as the base estimator and a Random Forest Classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest classifier will take a sample of features at each node, where as a bagging classifier will take a sample of features at to use for the whole model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state = 1, n_estimators=10000, max_depth=5, max_features=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, max_features=4, n_estimators=10000,\n",
       "                       random_state=1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at all the different default features\n",
    "rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, max_features=4, n_estimators=10000,\n",
       "                       random_state=1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model to the training data\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score:  0.763157894736842\n"
     ]
    }
   ],
   "source": [
    "#use the fitted model to predict on the test data\n",
    "rfc_preds = rfc.predict(X_test)\n",
    "\n",
    "rfc_f1 = metrics.f1_score(y_test, rfc_preds)\n",
    "\n",
    "# checking accuracy on the test data\n",
    "print('Test F1 score: ', rfc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Increase the number of trees and see how the model performs***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridsearchCV with Random Forest\n",
    "\n",
    "Let's use grid search to identify the best tuning parameters to use for a random forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary of all the parameters you want to tune\n",
    "param_grid = { \n",
    "    'n_estimators': [100,300,500,700,1000],\n",
    "#     'min_samples_split' : range(2,7) ,\n",
    "    'max_leaf_nodes' : range(18, 44, 2),\n",
    "#     'min_impurity_split': range(2,5),\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'min_samples_leaf': range(2,10),\n",
    "#     'max_features': ['auto', 5,6, None, 0.08]\n",
    "   \n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': list(range(2,10)),\n",
    "#     'max_features': list(range(3,7))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a grid search object and fit it to the data\n",
    "\n",
    "grid_tree=GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 65 candidates, totalling 325 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 325 out of 325 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'max_leaf_nodes': range(18, 44, 2),\n",
       "                         'n_estimators': [100, 300, 500, 700, 1000]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7546693073287798\n",
      "{'max_leaf_nodes': 36, 'n_estimators': 300}\n",
      "RandomForestClassifier(max_leaf_nodes=36, n_estimators=300)\n"
     ]
    }
   ],
   "source": [
    "### Identify the best params \n",
    "\n",
    "\n",
    "\n",
    "# Single best score achieved across all params (min_samples_split)\n",
    "print(grid_tree.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (min_samples_split) used to generate that score\n",
    "print(grid_tree.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid_tree.best_estimator_)\n",
    "#Identify the best score during fitting with cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7898089171974522\n"
     ]
    }
   ],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = grid_tree.best_estimator_.predict(X_test)\n",
    "\n",
    "# Model F1, how often is the classifier correct?\n",
    "print(\"F1:\",metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
